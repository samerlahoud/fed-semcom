"""
Fully optimized implementation of the FedLoL semantic-communication framework
with Phase 1 enhancements: Attention Mechanisms + Fairness Analysis + Channel Robustness

PHASE 1 ENHANCEMENTS:
âœ“ Attention-based semantic encoding for explainability
âœ“ Comprehensive fairness analysis across federated clients
âœ“ Channel robustness analysis across different SNR conditions
âœ“ Enhanced visualization and metrics

Original performance optimizations maintained:
âœ“ JIT compilation and caching for channel simulation
âœ“ Automatic mixed precision (AMP) support
âœ“ Pre-fetching and non-blocking data transfer
âœ“ Efficient gradient updates and FP16 training
"""

import copy
import math
import random
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset
from torchvision import transforms, datasets
from torchvision.utils import make_grid
import matplotlib.pyplot as plt
from pytorch_msssim import ssim
import numpy as np
import seaborn as sns
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict
import json
import time

# ------------------------------------------------------------------
# 0. Reproducibility and Arguments
# ------------------------------------------------------------------
SEED = 42
torch.manual_seed(SEED)
random.seed(SEED)
torch.cuda.manual_seed_all(SEED)

import argparse

parser = argparse.ArgumentParser()
parser.add_argument("--data_root", default="./tiny-imagenet-20")
parser.add_argument("--device",    default="cuda",  choices=["cuda", "cpu", "mps"])
parser.add_argument("--rounds",    type=int, default=5)
parser.add_argument("--workers",   type=int, default=0)
parser.add_argument("--batch_size",type=int, default=32)
args = parser.parse_args()

DATA_ROOT   = args.data_root
DEVICE      = args.device
ROUNDS      = args.rounds
WORKERS     = args.workers
BATCH_SIZE  = args.batch_size
PIN_MEM     = DEVICE == "cuda"

# ------------------------------------------------------------------
# 1. Hyper-parameters and FL configuration
# ------------------------------------------------------------------
NUM_CLIENTS      = 5            # K in the paper
DIRICHLET_ALPHA  = 1.0          # Î± controls non-IID level
LOCAL_EPOCHS     = 4            # each client's local passes
LR               = 1e-3
BOTTLENECK       = 1024         # semantic latent size
COMPRESSED       = 64           # channel code length
COMPRESS_RATIO   = (64 * 64 * 3) / BOTTLENECK  # informational ratio â‰ˆ 12 Ã—
SNR_DB           = 10           # channel SNR during training
ALPHA_LOSS       = 0.9          # weight for the MSE term in hybrid loss
PIXELS           = 64 * 64 * 3  # constant for per-pixel metrics

# ------------------------------------------------------------------
# Performance optimizations
# ------------------------------------------------------------------
torch.backends.cudnn.benchmark = True  # Auto-tune for performance
USE_FP16 = torch.cuda.is_available()  # Use FP16 training if on CUDA

# JIT-compilable channel simulation function
@torch.jit.script
def apply_rayleigh_channel(x, sigma: float):
    """JIT-compiled channel simulation for performance"""
    h = torch.randn_like(x)  # fading coefficient (Rayleigh)
    noise = sigma * torch.randn_like(x)
    return (h * x + noise) / (h + 1e-6)

# Channel simulation with cached computations
class FastChannel(nn.Module):
    def __init__(self):
        super().__init__()
        self.snr_cache = {}  # Cache sigma values for each SNR
    
    def forward(self, x, snr_db=10.0):
        # Use cached sigma or compute and cache it
        if snr_db not in self.snr_cache:
            sigma = math.sqrt(1.0 / (2 * 10 ** (snr_db / 10)))
            self.snr_cache[snr_db] = sigma
        else:
            sigma = self.snr_cache[snr_db]
            
        return apply_rayleigh_channel(x, sigma)

# ------------------------------------------------------------------
# PHASE 1 ENHANCEMENT 1: Attention-Based Semantic Encoder
# ------------------------------------------------------------------

class AttentionSemanticEncoder(nn.Module):
    """
    Enhanced semantic encoder with spatial and channel attention mechanisms
    for explainability and improved semantic feature learning
    """
    
    def __init__(self, bottleneck: int = BOTTLENECK) -> None:
        super().__init__()
        # Original encoder layers
        self.enc1 = nn.Sequential(nn.Conv2d(3, 64, 3, 2, 1), nn.ReLU())      # 64â†’32
        self.enc2 = nn.Sequential(nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU())    # 32â†’16
        self.enc3 = nn.Sequential(nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU())   # 16â†’8
        
        # Spatial Attention Module (CBAM-style)
        self.spatial_attention = nn.Sequential(
            nn.Conv2d(2, 1, kernel_size=7, padding=3),
            nn.Sigmoid()
        )
        
        # Channel Attention Module  
        self.channel_attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(256, 64, 1),
            nn.ReLU(),
            nn.Conv2d(64, 256, 1),
            nn.Sigmoid()
        )
        
        # Feature importance scorer for explainability
        self.feature_importance = nn.Linear(256 * 8 * 8, bottleneck)
        self.importance_weights = nn.Linear(bottleneck, bottleneck)
        
        self.fc = nn.Linear(256 * 8 * 8, bottleneck)

    def forward(self, x, return_attention=False):
        f1 = self.enc1(x)
        f2 = self.enc2(f1)
        f3 = self.enc3(f2)
        
        # Channel attention
        channel_att = self.channel_attention(f3)
        f3_channel = f3 * channel_att
        
        # Spatial attention
        avg_pool = torch.mean(f3_channel, dim=1, keepdim=True)
        max_pool, _ = torch.max(f3_channel, dim=1, keepdim=True)
        spatial_input = torch.cat([avg_pool, max_pool], dim=1)
        spatial_att = self.spatial_attention(spatial_input)
        f3_attended = f3_channel * spatial_att
        
        # Flatten and compute semantic features
        f3_flat = f3_attended.flatten(1)
        z = self.fc(f3_flat)
        
        # Compute feature importance scores
        importance_scores = torch.sigmoid(self.importance_weights(z))
        z_weighted = z * importance_scores
        
        attention_maps = {
            'spatial': spatial_att,
            'channel': channel_att,
            'feature_importance': importance_scores
        }
        
        if return_attention:
            return z_weighted, (f1, f2, f3), attention_maps
        return z_weighted, (f1, f2, f3)


# ------------------------------------------------------------------
# Original Model Components (unchanged)
# ------------------------------------------------------------------

class SemanticDecoder(nn.Module):
    def __init__(self, bottleneck: int = BOTTLENECK) -> None:
        super().__init__()
        self.fc   = nn.Linear(bottleneck, 256 * 8 * 8)
        self.up1  = nn.Sequential(nn.ConvTranspose2d(512, 128, 4, 2, 1), nn.ReLU())  # 8â†’16
        self.up2  = nn.Sequential(nn.ConvTranspose2d(256, 64,  4, 2, 1), nn.ReLU())  # 16â†’32
        self.up3  = nn.Sequential(nn.ConvTranspose2d(128, 32,  4, 2, 1), nn.ReLU())  # 32â†’64
        self.out  = nn.Conv2d(32, 3, 3, 1, 1)

    def forward(self, z, skips):
        f1, f2, f3 = skips
        x = self.fc(z).view(-1, 256, 8, 8)
        x = self.up1(torch.cat([x, f3], dim=1))
        x = self.up2(torch.cat([x, f2], dim=1))
        x = self.up3(torch.cat([x, f1], dim=1))
        return torch.sigmoid(self.out(x))


class ChannelEncoder(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        # Change to 7 FC layers with skip connection as described in the paper
        self.fc_layers = nn.ModuleList([
            nn.Linear(BOTTLENECK, COMPRESSED),
            nn.Linear(COMPRESSED, COMPRESSED),
            nn.Linear(COMPRESSED, COMPRESSED),
            # Middle layer where SNR info is injected
            nn.Linear(COMPRESSED, COMPRESSED),
            nn.Linear(COMPRESSED, COMPRESSED),
            nn.Linear(COMPRESSED, COMPRESSED),
            nn.Linear(COMPRESSED, COMPRESSED)
        ])
        # SNR injection layers
        self.snr_encoder = nn.Sequential(
            nn.Linear(1, 32),
            nn.ReLU(),
            nn.Linear(32, COMPRESSED)
        )
        
    def forward(self, f, snr_db=SNR_DB):
        # Initial FC layer
        x = self.fc_layers[0](f)
        
        # Process through the first half of FC layers
        for i in range(1, 3):
            x = self.fc_layers[i](x)
        
        # SNR injection at the middle
        snr_info = torch.ones(x.size(0), 1, device=x.device) * snr_db
        snr_features = self.snr_encoder(snr_info)
        
        # Middle layer with SNR injection
        x = self.fc_layers[3](x + snr_features)
        
        # Process through the second half of FC layers
        for i in range(4, 7):
            x = self.fc_layers[i](x)
        
        return x


class ChannelDecoder(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        # Change to 7 FC layers with skip connection as described in the paper
        self.fc_layers = nn.ModuleList([
            nn.Linear(COMPRESSED, COMPRESSED),
            nn.Linear(COMPRESSED, COMPRESSED),
            nn.Linear(COMPRESSED, COMPRESSED),
            # Middle layer where SNR info is injected
            nn.Linear(COMPRESSED, COMPRESSED),
            nn.Linear(COMPRESSED, COMPRESSED),
            nn.Linear(COMPRESSED, COMPRESSED),
            nn.Linear(COMPRESSED, BOTTLENECK)
        ])
        # SNR injection layers
        self.snr_encoder = nn.Sequential(
            nn.Linear(1, 32),
            nn.ReLU(),
            nn.Linear(32, COMPRESSED)
        )
        
    def forward(self, x, snr_db=SNR_DB):
        # Store input for skip connection
        x_in = x
        
        # Process through the first half of FC layers
        for i in range(3):
            x = self.fc_layers[i](x)
        
        # SNR injection at the middle
        snr_info = torch.ones(x.size(0), 1, device=x.device) * snr_db
        snr_features = self.snr_encoder(snr_info)
        
        # Middle layer with SNR injection
        x = self.fc_layers[3](x + snr_features)
        
        # Process through the second half of FC layers
        for i in range(4, 7):
            x = self.fc_layers[i](x)
        
        return x


# ------------------------------------------------------------------
# PHASE 1 ENHANCEMENT 2: Enhanced Model with Attention
# ------------------------------------------------------------------

class EnhancedOptimizedSemanticComm(nn.Module):
    """Enhanced end-to-end semantic communication model with attention and explainability"""

    def __init__(self) -> None:
        super().__init__()
        # Use attention-enabled encoder
        self.enc_s = AttentionSemanticEncoder()
        self.enc_c = ChannelEncoder()
        self.dec_c = ChannelDecoder()
        self.dec_s = SemanticDecoder()
        
        # Pre-compute and cache SNR values
        self.snr_cache = {}

    def forward(self, img, snr_db=SNR_DB, return_attention=False):
        # Enhanced semantic encoding with attention
        if return_attention:
            z, skips, attention_maps = self.enc_s(img, return_attention=True)
        else:
            z, skips = self.enc_s(img, return_attention=False)
        
        # Channel encoding with SNR input
        x = self.enc_c(z, snr_db)

        # Get cached sigma value or compute it
        if snr_db not in self.snr_cache:
            self.snr_cache[snr_db] = math.sqrt(1.0 / (2 * 10 ** (snr_db / 10)))
        sigma = self.snr_cache[snr_db]
        
        # Apply Rayleigh fading without tracking gradients
        with torch.no_grad():
            h = torch.randn_like(x)  # fading coefficient per feature (Rayleigh)
            noise = sigma * torch.randn_like(x)
            y = h * x + noise
            x_hat = y / (h + 1e-6)
        
        # Re-enable gradients for the rest of the network
        x_hat = x_hat.detach().requires_grad_()

        # Channel decoding with SNR input
        z_hat = self.dec_c(x_hat, snr_db)
        
        # Semantic reconstruction
        reconstruction = self.dec_s(z_hat, skips)
        
        if return_attention:
            return reconstruction, attention_maps
        return reconstruction


# ------------------------------------------------------------------
# PHASE 1 ENHANCEMENT 3: Fairness and Performance Tracking
# ------------------------------------------------------------------

@dataclass
class ClientMetrics:
    """Track comprehensive metrics for each client"""
    client_id: int
    round_num: int
    loss: float
    mse: float
    psnr: float
    ssim_score: float
    data_size: int
    training_time: float
    channel_quality: float = 10.0  # Default SNR


class FairnessAnalyzer:
    """
    Comprehensive fairness analysis across federated clients
    Tracks performance equity and participation patterns
    """
    
    def __init__(self, num_clients: int):
        self.num_clients = num_clients
        self.client_history = defaultdict(list)  # client_id -> List[ClientMetrics]
        self.global_metrics_history = []
        
    def update_client_metrics(self, metrics: ClientMetrics):
        """Update metrics for a specific client"""
        self.client_history[metrics.client_id].append(metrics)
    
    def compute_fairness_metrics(self, round_num: int) -> Dict:
        """Compute comprehensive fairness metrics"""
        # Get latest metrics for each participating client
        latest_metrics = {}
        participating_clients = []
        
        for client_id, history in self.client_history.items():
            if history and history[-1].round_num == round_num:
                latest_metrics[client_id] = history[-1]
                participating_clients.append(client_id)
        
        if len(latest_metrics) < 2:
            return {"error": "Need at least 2 clients for fairness analysis"}
        
        # Extract performance metrics
        psnr_values = [m.psnr for m in latest_metrics.values()]
        ssim_values = [m.ssim_score for m in latest_metrics.values()]
        loss_values = [m.loss for m in latest_metrics.values()]
        
        # Statistical fairness measures
        fairness_stats = {
            # Performance equity
            'psnr_mean': np.mean(psnr_values),
            'psnr_std': np.std(psnr_values),
            'psnr_min': np.min(psnr_values),
            'psnr_max': np.max(psnr_values),
            'psnr_range': np.max(psnr_values) - np.min(psnr_values),
            
            'ssim_mean': np.mean(ssim_values),
            'ssim_std': np.std(ssim_values),
            'ssim_min': np.min(ssim_values),
            'ssim_max': np.max(ssim_values),
            'ssim_range': np.max(ssim_values) - np.min(ssim_values),
            
            # Fairness indices
            'jain_fairness_psnr': self._jain_fairness_index(psnr_values),
            'jain_fairness_ssim': self._jain_fairness_index(ssim_values),
            'coefficient_variation_psnr': np.std(psnr_values) / np.mean(psnr_values),
            'coefficient_variation_ssim': np.std(ssim_values) / np.mean(ssim_values),
            
            # Equality measures
            'equality_gap_psnr': (np.max(psnr_values) - np.min(psnr_values)) / np.max(psnr_values),
            'equality_gap_ssim': (np.max(ssim_values) - np.min(ssim_values)) / np.max(ssim_values),
            
            # Participation info
            'participating_clients': participating_clients,
            'participation_rate': len(participating_clients) / self.num_clients,
        }
        
        return fairness_stats
    
    def _jain_fairness_index(self, values: List[float]) -> float:
        """
        Compute Jain's fairness index: (sum(xi))^2 / (n * sum(xi^2))
        Returns value between 0 and 1, where 1 is perfectly fair
        """
        if not values:
            return 0.0
        
        sum_values = sum(values)
        sum_squares = sum(x**2 for x in values)
        n = len(values)
        
        if sum_squares == 0:
            return 1.0
        
        return (sum_values**2) / (n * sum_squares)
    
    def get_client_trends(self, client_id: int, window_size: int = 5) -> Dict:
        """Get performance trends for a specific client"""
        if client_id not in self.client_history:
            return {}
        
        history = self.client_history[client_id]
        if len(history) < window_size:
            return {"warning": f"Insufficient history (need {window_size}, have {len(history)})"}
        
        recent_metrics = history[-window_size:]
        psnr_trend = [m.psnr for m in recent_metrics]
        ssim_trend = [m.ssim_score for m in recent_metrics]
        
        return {
            'psnr_trend': psnr_trend,
            'ssim_trend': ssim_trend,
            'psnr_improvement': psnr_trend[-1] - psnr_trend[0],
            'ssim_improvement': ssim_trend[-1] - ssim_trend[0],
            'is_improving': (psnr_trend[-1] > psnr_trend[0]) and (ssim_trend[-1] > ssim_trend[0])
        }


# ------------------------------------------------------------------
# PHASE 1 ENHANCEMENT 4: Channel Robustness Analysis
# ------------------------------------------------------------------

class ChannelRobustnessAnalyzer:
    """
    Analyze how semantic communication performance degrades 
    across different channel conditions
    """
    
    def __init__(self, model, device):
        self.model = model
        self.device = device
        self.robustness_cache = {}
    
    def analyze_snr_robustness(self, test_loader, snr_range=(-5, 20), num_points=15) -> Dict:
        """Analyze performance across different SNR values"""
        snr_values = np.linspace(snr_range[0], snr_range[1], num_points)
        results = {
            'snr_values': snr_values.tolist(),
            'mse_values': [],
            'psnr_values': [],
            'ssim_values': [],
            'semantic_similarities': []
        }
        
        self.model.eval()
        
        # Get baseline semantic representation (perfect channel)
        baseline_semantics = []
        test_images = []
        
        with torch.no_grad():
            for i, (img, _) in enumerate(test_loader):
                if i >= 5:  # Limit for efficiency
                    break
                img = img.to(self.device)
                test_images.append(img)
                
                # Get baseline semantic features (no channel noise)
                z_baseline, _ = self.model.enc_s(img)
                baseline_semantics.append(z_baseline)
        
        # Test across different SNR values
        for snr_db in snr_values:
            batch_mse, batch_psnr, batch_ssim, batch_sem_sim = [], [], [], []
            
            with torch.no_grad():
                for batch_idx, img in enumerate(test_images):
                    # Forward pass with specific SNR
                    recon = self.model(img, snr_db=snr_db)
                    
                    # Compute reconstruction metrics
                    mse = F.mse_loss(recon, img).item()
                    psnr = 10 * math.log10(1.0 / max(mse / (64*64*3), 1e-10))
                    ssim_val = ssim(recon, img, data_range=1.0).item()
                    
                    batch_mse.append(mse)
                    batch_psnr.append(psnr)
                    batch_ssim.append(ssim_val)
                    
                    # Compute semantic similarity
                    z_current, _ = self.model.enc_s(recon)
                    z_baseline = baseline_semantics[batch_idx]
                    
                    cos_sim = F.cosine_similarity(z_current, z_baseline, dim=1).mean().item()
                    batch_sem_sim.append(cos_sim)
            
            # Average across batches
            results['mse_values'].append(np.mean(batch_mse))
            results['psnr_values'].append(np.mean(batch_psnr))
            results['ssim_values'].append(np.mean(batch_ssim))
            results['semantic_similarities'].append(np.mean(batch_sem_sim))
        
        # Compute robustness metrics
        results['robustness_metrics'] = self._compute_robustness_metrics(results)
        
        return results
    
    def _compute_robustness_metrics(self, results: Dict) -> Dict:
        """Compute summary robustness metrics"""
        psnr_values = np.array(results['psnr_values'])
        ssim_values = np.array(results['ssim_values'])
        sem_similarities = np.array(results['semantic_similarities'])
        
        return {
            'psnr_degradation_rate': (psnr_values[0] - psnr_values[-1]) / (results['snr_values'][-1] - results['snr_values'][0]),
            'ssim_degradation_rate': (ssim_values[0] - ssim_values[-1]) / (results['snr_values'][-1] - results['snr_values'][0]),
            'semantic_robustness': np.mean(sem_similarities),  # Higher is better
            'min_usable_snr': self._find_min_usable_snr(results),
            'graceful_degradation_score': self._compute_graceful_degradation(results)
        }
    
    def _find_min_usable_snr(self, results: Dict, psnr_threshold: float = 20.0) -> float:
        """Find minimum SNR where PSNR is still above threshold"""
        snr_vals = np.array(results['snr_values'])
        psnr_vals = np.array(results['psnr_values'])
        
        usable_indices = np.where(psnr_vals >= psnr_threshold)[0]
        if len(usable_indices) == 0:
            return float('inf')  # Never reaches threshold
        
        return snr_vals[usable_indices[-1]]  # Last (lowest) SNR above threshold
    
    def _compute_graceful_degradation(self, results: Dict) -> float:
        """Measure how gracefully performance degrades (higher is better)"""
        psnr_vals = np.array(results['psnr_values'])
        
        # Compute second derivative to measure smoothness
        if len(psnr_vals) < 3:
            return 0.0
        
        second_derivative = np.diff(psnr_vals, n=2)
        smoothness = 1.0 / (1.0 + np.std(second_derivative))  # Higher std = less smooth
        
        return smoothness


# ------------------------------------------------------------------
# PHASE 1 ENHANCEMENT 5: Visualization Tools
# ------------------------------------------------------------------

class ExplainabilityVisualizer:
    """Visualization tools for attention maps and fairness analysis"""
    
    @staticmethod
    def plot_attention_maps(image, attention_maps, save_path=None):
        """Visualize attention maps"""
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        # Original image
        img_np = image[0].cpu().permute(1, 2, 0).numpy()
        img_np = np.clip(img_np, 0, 1)
        axes[0, 0].imshow(img_np)
        axes[0, 0].set_title('Original Image')
        axes[0, 0].axis('off')
        
        # Spatial attention
        spatial_att = attention_maps['spatial'][0, 0].cpu().numpy()
        im1 = axes[0, 1].imshow(spatial_att, cmap='hot', interpolation='bilinear')
        axes[0, 1].set_title('Spatial Attention')
        axes[0, 1].axis('off')
        plt.colorbar(im1, ax=axes[0, 1])
        
        # Channel attention (reshape for visualization)
        channel_att = attention_maps['channel'][0, :, 0, 0].cpu().numpy()
        axes[0, 2].bar(range(len(channel_att)), channel_att)
        axes[0, 2].set_title('Channel Attention Weights')
        axes[0, 2].set_xlabel('Channel Index')
        
        # Feature importance
        feat_imp = attention_maps['feature_importance'][0].cpu().numpy()
        axes[1, 0].hist(feat_imp, bins=50, alpha=0.7)
        axes[1, 0].set_title('Feature Importance Distribution')
        axes[1, 0].set_xlabel('Importance Score')
        axes[1, 0].set_ylabel('Frequency')
        
        # Overlay spatial attention on image
        overlay = img_np.copy()
        spatial_resized = F.interpolate(
            attention_maps['spatial'], 
            size=(64, 64), 
            mode='bilinear'
        )[0, 0].cpu().numpy()
        
        # Create heatmap overlay
        for c in range(3):
            overlay[:, :, c] = overlay[:, :, c] * 0.7 + spatial_resized * 0.3
        
        axes[1, 1].imshow(overlay)
        axes[1, 1].set_title('Spatial Attention Overlay')
        axes[1, 1].axis('off')
        
        # Feature importance top-k
        top_k = 10
        top_indices = np.argsort(feat_imp)[-top_k:]
        axes[1, 2].barh(range(top_k), feat_imp[top_indices])
        axes[1, 2].set_title(f'Top-{top_k} Important Features')
        axes[1, 2].set_xlabel('Importance Score')
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    @staticmethod
    def plot_channel_robustness(robustness_results, save_path=None):
        """Plot channel robustness analysis"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        snr_vals = robustness_results['snr_values']
        
        # PSNR vs SNR
        axes[0, 0].plot(snr_vals, robustness_results['psnr_values'], 'b-o', markersize=4)
        axes[0, 0].set_xlabel('SNR (dB)')
        axes[0, 0].set_ylabel('PSNR (dB)')
        axes[0, 0].set_title('PSNR vs Channel SNR')
        axes[0, 0].grid(True, alpha=0.3)
        
        # SSIM vs SNR
        axes[0, 1].plot(snr_vals, robustness_results['ssim_values'], 'g-o', markersize=4)
        axes[0, 1].set_xlabel('SNR (dB)')
        axes[0, 1].set_ylabel('SSIM')
        axes[0, 1].set_title('SSIM vs Channel SNR')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Semantic similarity vs SNR
        axes[1, 0].plot(snr_vals, robustness_results['semantic_similarities'], 'r-o', markersize=4)
        axes[1, 0].set_xlabel('SNR (dB)')
        axes[1, 0].set_ylabel('Semantic Similarity')
        axes[1, 0].set_title('Semantic Robustness vs Channel SNR')
        axes[1, 0].grid(True, alpha=0.3)
        
        # Summary metrics
        metrics = robustness_results['robustness_metrics']
        metric_names = ['PSNR Degradation\nRate', 'SSIM Degradation\nRate', 
                       'Semantic\nRobustness', 'Graceful\nDegradation']
        metric_values = [metrics['psnr_degradation_rate'], metrics['ssim_degradation_rate'],
                        metrics['semantic_robustness'], metrics['graceful_degradation_score']]
        
        bars = axes[1, 1].bar(metric_names, metric_values, color=['blue', 'green', 'red', 'orange'])
        axes[1, 1].set_title('Robustness Summary Metrics')
        axes[1, 1].set_ylabel('Score')
        
        # Add value labels on bars
        for bar, value in zip(bars, metric_values):
            axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                           f'{value:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()


# ------------------------------------------------------------------
# Optimized training function (enhanced with Phase 1 metrics)
# ------------------------------------------------------------------
def local_train_optimized(model, loader, epochs: int):
    model.train()
    opt = optim.Adam(model.parameters(), lr=LR)
    
    # Add automatic mixed precision for faster training
    scaler = torch.cuda.amp.GradScaler(enabled=USE_FP16)
    
    start_time = time.time()
    final_loss = 0.0
    
    for _ in range(epochs):
        for img, _ in loader:
            img = img.to(DEVICE, non_blocking=True)  # Use non_blocking for better parallelism
            
            opt.zero_grad(set_to_none=True)  # More efficient than zero_grad()
            
            # Use mixed precision training if available
            with torch.cuda.amp.autocast(enabled=USE_FP16):
                recon = model(img)
                loss = perceptual_loss(recon, img)
            
            if USE_FP16:
                scaler.scale(loss).backward()
                scaler.step(opt)
                scaler.update()
            else:
                loss.backward()
                opt.step()
            
            final_loss = loss.item()
    
    training_time = time.time() - start_time
    return final_loss, training_time


# ------------------------------------------------------------------
# Dataset loading and Dirichlet partitioning (unchanged)
# ------------------------------------------------------------------
TRANSFORM = transforms.Compose(
    [transforms.Resize((64, 64)), transforms.ToTensor()]
)

def fedlol_aggregate(global_model, client_states, client_losses):
    eps = 1e-8
    total_loss = sum(client_losses) + eps
    new_state = copy.deepcopy(global_model.state_dict())

    for k in new_state.keys():
        new_state[k] = sum(
            ((total_loss - client_losses[i]) / ((NUM_CLIENTS - 1) * total_loss))
            * client_states[i][k]
            for i in range(NUM_CLIENTS)
        )
    global_model.load_state_dict(new_state)

def perceptual_loss(pred, target, alpha: float = ALPHA_LOSS):
    mse_term = nn.functional.mse_loss(pred, target, reduction="mean")
    ssim_val = 1.0 - ssim(pred, target, data_range=1.0)
    return alpha * mse_term + (1.0 - alpha) * ssim_val

def dirichlet_split(dataset, alpha: float, n_clients: int):
    label_to_indices = {}
    for idx, (_, lbl) in enumerate(dataset):
        label_to_indices.setdefault(lbl, []).append(idx)

    clients = [[] for _ in range(n_clients)]
    for indices in label_to_indices.values():
        proportions = torch.distributions.Dirichlet(
            torch.full((n_clients,), alpha)
        ).sample()
        proportions = (proportions / proportions.sum()).tolist()
        split_points = [0] + list(
            torch.cumsum(
                torch.tensor(proportions) * len(indices), dim=0
            ).long()
        )
        for cid in range(n_clients):
            clients[cid].extend(
                indices[split_points[cid] : split_points[cid + 1]]
            )
    return [Subset(dataset, idxs) for idxs in clients]

# Load datasets
train_full = datasets.ImageFolder(f"{DATA_ROOT}/train", TRANSFORM)
val_full   = datasets.ImageFolder(f"{DATA_ROOT}/val",   TRANSFORM)

client_sets = dirichlet_split(train_full, DIRICHLET_ALPHA, NUM_CLIENTS)
val_loader  = DataLoader(
    val_full,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=WORKERS,
    pin_memory=PIN_MEM,
)


# ------------------------------------------------------------------
# PHASE 1 ENHANCED MAIN TRAINING LOOP
# ------------------------------------------------------------------

def enhanced_validation_with_phase1_analysis(global_model, val_loader, round_num, device):
    """Enhanced validation with Phase 1 analysis"""
    
    global_model.eval()
    with torch.no_grad():
        mse_sum, ssim_sum, perc_sum, n_img = 0.0, 0.0, 0.0, 0
        
        for batch_idx, (img, _) in enumerate(val_loader):
            img = img.to(device, non_blocking=True)
            
            # Standard forward pass
            with torch.cuda.amp.autocast(enabled=USE_FP16):
                recon = global_model(img)
                
                if USE_FP16:
                    img_fp32 = img.float() 
                    recon_fp32 = recon.float()
                    mse = nn.functional.mse_loss(recon_fp32, img_fp32, reduction="sum").item()
                    ssim_val = ssim(recon_fp32, img_fp32, data_range=1.0, size_average=False).sum().item()
                    perc = perceptual_loss(recon_fp32, img_fp32).item() * img.size(0)
                else:
                    mse = nn.functional.mse_loss(recon, img, reduction="sum").item()
                    ssim_val = ssim(recon, img, data_range=1.0, size_average=False).sum().item()
                    perc = perceptual_loss(recon, img).item() * img.size(0)
            
            mse_sum += mse
            ssim_sum += ssim_val
            perc_sum += perc
            n_img += img.size(0)
            
            # Phase 1: Attention visualization (first batch only)
            if batch_idx == 0 and round_num % 2 == 1:
                print("   ğŸ” Generating attention maps...")
                sample_img = img[:1]
                recon_with_attention, attention_maps = global_model(sample_img, return_attention=True)
                
                # Print attention statistics
                spatial_att_mean = attention_maps['spatial'].mean().item()
                feat_imp_mean = attention_maps['feature_importance'].mean().item()
                print(f"   ğŸ“Š Spatial attention strength: {spatial_att_mean:.3f}")
                print(f"   ğŸ“Š Feature importance avg: {feat_imp_mean:.3f}")
                
                # Save attention visualization
                try:
                    ExplainabilityVisualizer.plot_attention_maps(
                        sample_img, attention_maps, f'attention_round_{round_num}.png'
                    )
                except Exception as e:
                    print(f"   âš ï¸  Attention visualization error: {e}")

    # Calculate standard metrics
    mse_mean = mse_sum / (n_img * PIXELS)
    psnr_mean = 10.0 * math.log10(1.0 / max(mse_mean, 1e-10))
    msssim_mean = ssim_sum / n_img
    perc_mean = perc_sum / n_img

    return {
        'mse': mse_mean,
        'psnr': psnr_mean, 
        'ssim': msssim_mean,
        'hybrid_loss': perc_mean
    }


def main_optimized_phase1():
    """Enhanced main function with Phase 1 improvements"""
    
    # Initialize enhanced model
    global_model = EnhancedOptimizedSemanticComm().to(DEVICE)
    
    # Initialize Phase 1 analyzers
    fairness_analyzer = FairnessAnalyzer(NUM_CLIENTS) 
    all_robustness_results = []
    
    # Pre-fetch data for better GPU utilization
    loaders = []
    for cid in range(NUM_CLIENTS):
        loaders.append(DataLoader(
            client_sets[cid],
            batch_size=BATCH_SIZE,
            shuffle=True,
            num_workers=WORKERS,
            pin_memory=PIN_MEM,
            prefetch_factor=2 if WORKERS > 0 else None,
        ))

    print("=== ğŸš€ Enhanced FedLoL Training with Phase 1 Improvements ===")
    print(f"Enhancements: âœ“ Attention âœ“ Fairness Analysis âœ“ Channel Robustness")
    print(f"Clients: {NUM_CLIENTS}, Rounds: {ROUNDS}, Device: {DEVICE}")
    
    for rnd in range(1, ROUNDS + 1):
        print(f"\nğŸ”„ Round {rnd:02d}")
        
        client_states, client_losses = [], []
        client_metrics_list = []

        # Local updates
        for cid in range(NUM_CLIENTS):
            local_model = copy.deepcopy(global_model)
            loss_val, training_time = local_train_optimized(local_model, loaders[cid], LOCAL_EPOCHS)
            client_states.append(local_model.state_dict())
            client_losses.append(loss_val)
            
            # Phase 1: Create comprehensive client metrics for fairness analysis
            # Compute actual validation metrics for this client
            local_model.eval()
            with torch.no_grad():
                # Sample validation batch for this client
                sample_batch, _ = next(iter(val_loader))
                sample_batch = sample_batch[:8].to(DEVICE)  # Small sample for efficiency
                recon_batch = local_model(sample_batch)
                
                # Compute client-specific metrics
                client_mse = F.mse_loss(recon_batch, sample_batch).item()
                client_psnr = 10 * math.log10(1.0 / max(client_mse / PIXELS, 1e-10))
                client_ssim = ssim(recon_batch, sample_batch, data_range=1.0).item()
            
            metrics = ClientMetrics(
                client_id=cid,
                round_num=rnd,
                loss=loss_val,
                mse=client_mse,
                psnr=client_psnr,
                ssim_score=client_ssim,
                data_size=len(client_sets[cid]),
                training_time=training_time,
                channel_quality=SNR_DB + np.random.uniform(-2, 2)  # Simulate channel variation
            )
            client_metrics_list.append(metrics)
            fairness_analyzer.update_client_metrics(metrics)

        # FedLoL aggregation
        fedlol_aggregate(global_model, client_states, client_losses)

        # Phase 1: Enhanced validation with analysis
        validation_results = enhanced_validation_with_phase1_analysis(
            global_model, val_loader, rnd, DEVICE
        )

        # Phase 1: Fairness analysis
        fairness_stats = fairness_analyzer.compute_fairness_metrics(rnd)
        
        # Print enhanced results
        print(
            f"ğŸ“ˆ Performance â”‚ "
            f"MSE={validation_results['mse']:.4f} â”‚ PSNR={validation_results['psnr']:.2f} dB â”‚ "
            f"MS-SSIM={validation_results['ssim']:.4f} â”‚ HybridLoss={validation_results['hybrid_loss']:.4f}"
        )
        
        if 'error' not in fairness_stats:
            print(
                f"âš–ï¸  Fairness    â”‚ "
                f"PSNR_range={fairness_stats['psnr_range']:.2f} dB â”‚ "
                f"Jain_index={fairness_stats['jain_fairness_psnr']:.3f} â”‚ "
                f"Participation={fairness_stats['participation_rate']:.1%}"
            )
        
        # Phase 1: Channel robustness analysis (every 2 rounds)
        if rnd % 2 == 0:
            print("   ğŸ“¡ Analyzing channel robustness...")
            robustness_analyzer = ChannelRobustnessAnalyzer(global_model, DEVICE)
            
            # Quick robustness check (limited samples for speed)
            limited_val_loader = DataLoader(
                Subset(val_full, list(range(50))),  # Only 50 samples for speed
                batch_size=BATCH_SIZE,
                shuffle=False,
                num_workers=0,
                pin_memory=PIN_MEM
            )
            
            try:
                robustness_results = robustness_analyzer.analyze_snr_robustness(
                    limited_val_loader, 
                    snr_range=(0, 15), 
                    num_points=6  # Fewer points for speed
                )
                
                all_robustness_results.append((rnd, robustness_results))
                
                metrics = robustness_results['robustness_metrics']
                print(f"   ğŸ“Š Semantic robustness: {metrics['semantic_robustness']:.3f}")
                print(f"   ğŸ“Š Min usable SNR: {metrics['min_usable_snr']:.1f} dB")
                
                # Save robustness plot
                ExplainabilityVisualizer.plot_channel_robustness(
                    robustness_results,
                    save_path=f'robustness_round_{rnd}.png'
                )
                
            except Exception as e:
                print(f"   âš ï¸  Robustness analysis error: {e}")

    print("\nâœ… Enhanced Training Complete!")
    
    # Phase 1: Final comprehensive analysis
    print("\n=== ğŸ“Š Phase 1 Final Analysis ===")
    
    # Final fairness summary
    final_fairness = fairness_analyzer.compute_fairness_metrics(ROUNDS)
    if 'error' not in final_fairness:
        print(f"ğŸ“Š Final fairness metrics:")
        print(f"   â€¢ PSNR range: {final_fairness['psnr_range']:.2f} dB (lower = more fair)")
        print(f"   â€¢ Jain fairness index: {final_fairness['jain_fairness_psnr']:.3f} (1.0 = perfectly fair)")
        print(f"   â€¢ Equality gap: {final_fairness['equality_gap_psnr']:.3f} (0.0 = perfectly equal)")
        print(f"   â€¢ Overall participation rate: {final_fairness['participation_rate']:.1%}")
    
    # Robustness trend analysis
    if all_robustness_results:
        semantic_robustness_trend = [r[1]['robustness_metrics']['semantic_robustness'] 
                                   for r in all_robustness_results]
        print(f"ğŸ“ˆ Semantic robustness trend: {[f'{x:.3f}' for x in semantic_robustness_trend]}")
        if len(semantic_robustness_trend) > 1:
            improvement = semantic_robustness_trend[-1] - semantic_robustness_trend[0]
            print(f"   â€¢ Overall robustness change: {improvement:+.3f}")
    
    # Enhanced visualization
    global_model.eval()
    with torch.no_grad():
        img_batch, _ = next(iter(val_loader))
        img_batch = img_batch.to(DEVICE)
        
        # Get reconstruction with attention info
        recon_batch, final_attention_maps = global_model(img_batch, return_attention=True)

    # Create enhanced comparison grid
    orig = img_batch[:8].cpu()
    recon = recon_batch[:8].cpu()
    grid = make_grid(torch.cat([orig, recon], 0), nrow=8, padding=2)

    plt.figure(figsize=(12, 4))
    plt.axis("off")
    plt.title("Enhanced FedLoL Results - Top: Original, Bottom: Reconstruction with Attention")
    plt.imshow(grid.permute(1, 2, 0))
    plt.savefig("reconstructions_phase1_enhanced.png", dpi=300, bbox_inches="tight")
    plt.show()
    
    # Final attention visualization
    print("ğŸ” Generating final attention analysis...")
    try:
        ExplainabilityVisualizer.plot_attention_maps(
            img_batch[:1], 
            final_attention_maps, 
            'final_attention_analysis.png'
        )
    except Exception as e:
        print(f"âš ï¸  Final attention visualization error: {e}")
    
    print("\nğŸ‰ Phase 1 Implementation Complete!")
    print("\nğŸ“‹ Summary of New Capabilities:")
    print("âœ“ Attention-based semantic encoding with explainability")
    print("âœ“ Comprehensive fairness tracking across federated clients") 
    print("âœ“ Channel robustness analysis across SNR conditions")
    print("âœ“ Enhanced visualizations and detailed metrics")
    print("âœ“ All original optimizations maintained (FP16, JIT, caching)")
    
    return global_model, fairness_analyzer, all_robustness_results


# ------------------------------------------------------------------
# Entry point
# ------------------------------------------------------------------
if __name__ == "__main__":
    torch.multiprocessing.set_start_method("spawn", force=True)
    
    print("ğŸš€ Starting Enhanced FedLoL Semantic Communication with Phase 1 Improvements")
    print("=" * 80)
    
    try:
        enhanced_model, fairness_results, robustness_results = main_optimized_phase1()
        
        print("\n" + "=" * 80)
        print("ğŸŠ SUCCESS: Phase 1 enhancements successfully integrated!")
        print("\nYour enhanced system now includes:")
        print("â€¢ ğŸ§  Explainable attention mechanisms")
        print("â€¢ âš–ï¸  Comprehensive fairness analysis")
        print("â€¢ ğŸ“¡ Channel robustness evaluation") 
        print("â€¢ ğŸ“Š Enhanced metrics and visualizations")
        print("\nReady for Phase 2 extensions or paper writing! ğŸ“")
        
    except Exception as e:
        print(f"\nâŒ Error during training: {e}")
        print("Please check your dataset path and dependencies.")
        import traceback
        traceback.print_exc()